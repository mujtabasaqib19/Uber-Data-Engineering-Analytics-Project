ğŸš€ Uber Data Pipeline with Airflow, dbt & Snowflake
This project implements a data pipeline using:

Apache Airflow (for orchestration)

dbt (for transformations)

Snowflake (as the data warehouse)

Astronomer Cosmos (for integrating dbt in Airflow)

It processes Uber trip data to generate business-ready marts for analytics and Power BI dashboards.

ğŸ“‚ Project Structure

.
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ uberdag.py         # Airflow DAG using Cosmos
â”‚   â”‚   â””â”€â”€ uberdbt/           # dbt project (models, seeds, etc.)
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt       # Airflow & Cosmos dependencies
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ profiles.yml
â”œâ”€â”€ .env                        # Environment variables for Airflow & Snowflake
â”œâ”€â”€ docker-compose.yml          # Orchestrates Airflow with dbt
â””â”€â”€ README.md
ğŸ›  Prerequisites
âœ… Install Docker & Docker Compose
âœ… Snowflake account (with warehouse & schema created)
âœ… Python 3.8+ (for local dbt testing)

âš™ï¸ Setup
1ï¸âƒ£ Clone the repository
bash
Copy
Edit
git clone https://github.com/<your-username>/uber-data-pipeline.git
cd uber-data-pipeline
2ï¸âƒ£ Configure Airflow connection for Snowflake
In Airflow UI:

Go to Admin â†’ Connections â†’ New

Add a Snowflake connection:

Field	Value
Conn Id	snowflake_conn
Conn Type	Snowflake
Account	your_account.region (e.g. jc92948.ap-south-1)
Warehouse	COMPUTE_WH
Database	dbt_db
Schema	dbt_schema
Login	Your Snowflake username
Password	Your Snowflake password

3ï¸âƒ£ Spin up Airflow
bash
Copy
Edit
docker-compose up -d
Access Airflow at http://localhost:8080
(Default user: airflow, password: airflow)

4ï¸âƒ£ Initialize dbt
Inside the Airflow container:

bash
Copy
Edit
docker exec -it <scheduler-container> bash
cd /usr/local/airflow/dags/uberdbt
dbt deps  # Install dbt dependencies
ğŸš¦ Running the Pipeline
âœ… Trigger the DAG in Airflow UI: dbt_snowflake_pipeline

This will:

Run dbt transformations (staging â†’ intermediate â†’ marts).

Load marts into Snowflake for analytics.

ğŸ“Š BI Dashboard
Once data is in Snowflake, you can connect Power BI/Tableau directly to:

Trips mart: Detailed trip-level data

Vendors mart: Aggregated metrics by vendor

Payments mart: Payment type breakdown

ğŸ§ª Testing
Run dbt tests:

bash
Copy
Edit
dbt test --profiles-dir profiles
ğŸ”‘ Key Technologies
Apache Airflow: Workflow orchestration

dbt: Data transformations & models

Snowflake: Cloud data warehouse

Astronomer Cosmos: Native dbt + Airflow integration

ğŸš€ Next Improvements
Incremental dbt models for large datasets

Role-based Airflow connections

Deploy on AWS ECS / GCP Composer
