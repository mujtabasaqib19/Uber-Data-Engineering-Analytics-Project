ğŸš€ Uber Data Pipeline: Airflow + dbt + Snowflake
This project implements a modern data pipeline for processing Uber trip data. It uses Apache Airflow for orchestration, dbt for data transformations, and Snowflake as the cloud data warehouse. The pipeline creates analytics-ready marts that can be used for Power BI or Tableau dashboards.

ğŸ“‚ Project Structure
text
Copy
Edit
.
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ uberdag.py          # Airflow DAG (Cosmos integrated)
â”‚   â”‚   â””â”€â”€ uberdbt/            # dbt project (models, seeds, etc.)
â”‚   â”œâ”€â”€ Dockerfile              # Airflow container setup
â”‚   â””â”€â”€ requirements.txt        # Airflow & Cosmos dependencies
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ dbt_project.yml         # dbt project config
â”‚   â”œâ”€â”€ models/                 # dbt models: staging, intermediate, marts
â”‚   â””â”€â”€ profiles.yml            # dbt profiles
â”œâ”€â”€ docker-compose.yml          # Orchestrates Airflow + dbt
â”œâ”€â”€ .env                        # Environment variables for Snowflake
â””â”€â”€ README.md                   # Project documentation
ğŸ› ï¸ Prerequisites
âœ… Docker & Docker Compose installed
âœ… Snowflake account with database and schema configured
âœ… Python 3.8+ for local dbt testing

âš™ï¸ Setup
1ï¸âƒ£ Clone the Repository
bash
Copy
Edit
git clone https://github.com/<your-username>/uber-data-pipeline.git
cd uber-data-pipeline
2ï¸âƒ£ Configure Airflow Snowflake Connection
In Airflow UI:

Go to Admin â†’ Connections â†’ New

Add a Snowflake connection:

Field	Value
Conn Id	snowflake_conn
Conn Type	Snowflake
Account	jc92948.ap-south-1 (your account locator)
Warehouse	COMPUTE_WH
Database	dbt_db
Schema	dbt_schema
Login	Your Snowflake username
Password	Your Snowflake password

3ï¸âƒ£ Start Airflow
bash
Copy
Edit
docker-compose up -d
Access Airflow at http://localhost:8080
(Default credentials: airflow / airflow)

4ï¸âƒ£ Install dbt Dependencies
Inside the Airflow container:

bash
Copy
Edit
docker exec -it <scheduler-container> bash
cd /usr/local/airflow/dags/uberdbt
dbt deps
ğŸš¦ Running the Pipeline
Trigger the DAG in Airflow UI:
âœ… dbt_snowflake_pipeline

This will:

Run dbt models in Snowflake (staging â†’ intermediate â†’ marts).

Generate analytics-ready marts for BI dashboards.

ğŸ“Š Analytics Marts
Mart	Description
trips	Trip-level data with derived features & aggregates
vendors	Vendor-level summaries (total trips, revenue, etc.)
payments	Payment method breakdowns (tips, fares, surcharges)

These marts can be consumed directly in Power BI, Tableau, or Looker.

ğŸ§ª Testing
Run dbt tests inside the container:

bash
Copy
Edit
dbt test --profiles-dir profiles
ğŸ›  Technologies Used
Apache Airflow â€“ Orchestration

dbt â€“ Transformations and data modeling

Snowflake â€“ Cloud data warehouse

Astronomer Cosmos â€“ Airflow + dbt integration

